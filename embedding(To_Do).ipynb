{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意(attention)！开始做前必读项！\n",
    "所有的代码一定要在这个文件里面编写，不要自己创建一个新的文件 对于提供的数据集，不要改存储地方，也不要修改文件名和内容 不要重新定义函数（如果我们已经定义好的），按照里面的思路来编写。当然，除了我们定义的部分，如有需要可以自行定义函数或者模块 写完之后，重新看一下哪一部分比较慢，然后试图去优化。一个好的习惯是每写一部分就思考这部分代码的时间复杂度和空间复杂度，AI工程是的日常习惯！ \n",
    "这次作业很重要，一定要完成！ 相信会有很多的收获！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练集文件和测试集文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text,data=[],[]\n",
    "stopWords=[]\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    函数说明：该函数用于加载数据集\n",
    "    return: \n",
    "        -data: 表示所有数据拼接的原始数据\n",
    "        -data_text: 表示数据集中的特征数据集\n",
    "        -datatext: 表示经过分词之后的特征数据集\n",
    "        -stopWords: 表示读取的停用词\n",
    "    '''\n",
    "    print('load Pre_process')\n",
    "    data = pd.concat([\n",
    "        pd.read_csv('train_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('dev_clean.csv', sep='\\t'),\n",
    "        pd.read_csv('test_clean.csv', sep='\\t')\n",
    "        ])\n",
    "    print(\"读取数据集完成\")\n",
    "    data_text = list(data.text)  # .apply(lambda x: x.split(' '))\n",
    "    datatext = []\n",
    "    for i in range(len(data_text)):\n",
    "        datatext.append(data_text[i].split(' '))\n",
    "    stopWords = open('stopwords(1).txt', 'r', encoding='utf-8').readlines()\n",
    "    print(\"取停用词完成\")\n",
    "    return data, data_text,datatext, stopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Pre_process\n",
      "读取数据集完成\n",
      "取停用词完成\n"
     ]
    }
   ],
   "source": [
    "data, data_text, datatext, stopWords=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_tfidf(data, stopWords):\n",
    "    '''\n",
    "    函数说明：该函数用于训练tfidf的词向量\n",
    "    return: \n",
    "        -tfidf: 表示经过TF-ID模型训练出的词向量\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 根据数据集训练tfidf的词向量\n",
    "    # 第一步：首先通过TfidfVectorizer创建一个模型对象\n",
    "    count_vect = TfidfVectorizer(stop_words=stopWords, max_df=0.6, ngram_range=(1,2))\n",
    "    # 第二步：用模型对象去fit训练数据集\n",
    "    tfidf = count_vect.fit(data)\n",
    "    print('train tfidf_embedding')\n",
    "    #返回是一个稀疏矩阵\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tfidf_embedding\n"
     ]
    }
   ],
   "source": [
    "tfidf = trainer_tfidf(data_text, stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf.transform(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据数据集训练word2vec的词向量  \n",
    "利用models.Word2Vec训练w2v的词向量  \n",
    "部分参数说明：  \n",
    "min_count：表示低于该频率的词将会删除，  \n",
    "window：表示滑动窗口大小，  \n",
    "alpha：表示学习率，  \n",
    "negative：表示负采样样本个数，  \n",
    "max_vocab_size：表示RAM中最大限制的词个数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_w2v():\n",
    "    '''\n",
    "    函数说明：该函数基于Word2vec模型训练词向量\n",
    "    return: \n",
    "        -w2v: 表示经过word2vec模型训练出的词向量\n",
    "    '''\n",
    "    print('train word2vec Embedding')\n",
    "    # 训练w2v的词向量\n",
    "    # 第一步：利用models.Word2Vec构建一个模型，\n",
    "    w2v = models.Word2Vec(min_count=2,\n",
    "                                window=3,\n",
    "                                vector_size=300,\n",
    "                                sample=6e-5,\n",
    "                                alpha=0.03,\n",
    "                                min_alpha=0.0007,\n",
    "                                negative=15,\n",
    "                                workers=4,\n",
    "                                max_vocab_size=50000) \n",
    "        \n",
    "    w2v.build_vocab(datatext)\n",
    "\n",
    "    w2v.train(datatext,\n",
    "              total_examples=w2v.corpus_count,\n",
    "              epochs=15,\n",
    "              report_delay=1)\n",
    "        \n",
    "    print('train fast_embedding')\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word2vec Embedding\n",
      "train fast_embedding\n"
     ]
    }
   ],
   "source": [
    "w2v = trainer_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_fasttext():\n",
    "    '''\n",
    "    函数说明：该函数基于FastText模型训练词向量\n",
    "    return: \n",
    "        -fast: 表示经过FastText模型训练出的词向量\n",
    "    '''\n",
    "    # ToDo\n",
    "    # 根据数据集训练FastTExt的词向量\n",
    "    # hint: 利用models.FastText,\n",
    "    # 可使用部分参数说明：\n",
    "    # vector_size：生成的向量维度，\n",
    "    # window: 移动窗口，\n",
    "    # aphla: 学习率，\n",
    "    # min_count: 对低于该频率的词进行截断\n",
    "    # 可以参照trainer_w2v函数完成FastText的词向量的训练\n",
    "    # 可以直接根据models.FastText直接训练样本获取词向量\n",
    "    fast = models.FastText(vector_size=300,\n",
    "                           window=3,\n",
    "                           alpha=0.025,\n",
    "                           min_count=2)\n",
    "    fast.build_vocab(datatext)\n",
    "    fast.train(datatext, total_examples=fast.corpus_count, epochs=10)\n",
    "    return fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast= trainer_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save tfidf model\n",
      "save word2vec model\n",
      "save fast model\n"
     ]
    }
   ],
   "source": [
    "def saver():\n",
    "    '''\n",
    "    函数说明：该函数存储训练好的模型\n",
    "    '''\n",
    "    #Todo\n",
    "    # hint: 通过joblib.dump保存tfidf\n",
    "    print('save tfidf model')\n",
    "    joblib.dump(tfidf, './tfidf_model')\n",
    "    # hint: w2v可以通过自带的save函数进行保存\n",
    "    w2v.save('./w2v_model')\n",
    "    print('save word2vec model')\n",
    "     # hint: fast可以通过自带的save函数进行保存\n",
    "    fast.save('./fast_model')\n",
    "    print('save fast model')\n",
    "\n",
    "    \n",
    "saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tfidf_embedding model\n",
      "load w2v_embedding model\n",
      "load fast_embedding model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-810e95aa6a57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load fast_embedding model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfast\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./fast_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-810e95aa6a57>\u001b[0m in \u001b[0;36mload\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./w2v_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load fast_embedding model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfast\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./fast_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ananconda\\envs\\data\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorted_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mnull_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnull_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_exponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mns_exponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashfxn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_post_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ananconda\\envs\\data\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab)\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=self.compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ananconda\\envs\\data\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ananconda\\envs\\data\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "def load():\n",
    "    '''\n",
    "    函数说明：该函数加载训练好的模型\n",
    "    '''\n",
    "    #ToDo\n",
    "    # 加载模型 \n",
    "    # hint: tfidf可以通过joblib.load进行加载\n",
    "    # w2v和fast可以通过gensim.models.KeyedVectors.load加载\n",
    "    print('load tfidf_embedding model')\n",
    "    tfidf = joblib.load('./tfidf_model')\n",
    "    print('load w2v_embedding model')\n",
    "    w2v =  models.Word2Vec.load('./w2v_model')\n",
    "    print('load fast_embedding model')\n",
    "    fast =  models.FastText('./fast_model')\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data)",
   "language": "python",
   "name": "data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
